{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB-D Fusion\n",
    "\n",
    "This exercise will describe how to fuse images from known poses into one pointcloud.\n",
    "The excercise is divided into the following steps.\n",
    "\n",
    "1. Load recorded images and poses.\n",
    "2. \n",
    "\n",
    "Note: if the interactive viewer does not work you may have to restart the notebook with `%matplotlib widget` instead of `%matplotlib notebook`\n",
    "\n",
    "Note: install jupyter in your conda environment via `conda install -c conda-forge notebook` and the other missing packages. Start it in the command line via` jupter notebook --port xxxx`. Use the vs-code pop up to open jupyter in your local server.\n",
    "\n",
    "Note: we use [Open3D](http://www.open3d.org/) for this exercise. Unfortunately the visualization requires OpenGL, which fails for setups like ours, where the visualization and the code run on separate machines. A minimal working solution (with drawbacks in quality) is to start a jupyter notebook on one of the pool machines (not the login) and use vscode for port-forwarding it to your local browser.\n",
    "**The recommended way is to run everything on your local machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# %matplotlib widget\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "try:\n",
    "    import open3d as o3d\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data required for this exercise (30Mb)\n",
    "! wget https://lmb.informatik.uni-freiburg.de/people/argusm/wd_40.tar\n",
    "! tar -xvf wd_40.tar\n",
    "# or find them here\n",
    "! ls /project/cv-ws2122/shared-data1/wd_40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Recorded Images\n",
    "\n",
    "This cell defines a data class that loads images and data from files.\n",
    "\n",
    "Please complete the `get_projection_matrix` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "class ViewLoader:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        assert os.path.isdir(base_path)\n",
    "        files = sorted(os.listdir(self.base_path))\n",
    "        files = [f for f in files if (f.startswith(\"rgb_\") and f.endswith(\".png\"))]\n",
    "        self.max_idx = int(files[-1].replace(\"rgb_\", \"\").replace(\".png\", \"\"))\n",
    "        print(f\"Loaded {self.max_idx+1} images.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.max_idx + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_rgbdp(idx)\n",
    "\n",
    "    def get_info(self):\n",
    "        info = {\n",
    "            \"camera\": {\"calibration\": \n",
    "            {\"width\": 640, \"height\": 480,\n",
    "            \"fx\": 617.8902587890625, \"fy\": 617.8903198242188,\n",
    "            \"ppx\": 315.20367431640625, \"ppy\": 245.70614624023438}}\n",
    "        }\n",
    "        return info\n",
    "    \n",
    "    def get_intrinsics(self):\n",
    "        info = self.get_info()\n",
    "        calib = info[\"camera\"][\"calibration\"]\n",
    "        return calib\n",
    "    \n",
    "    def get_K(self):\n",
    "        calib = self.get_intrinsics()\n",
    "        cam_intrinsic = np.eye(3)\n",
    "        cam_intrinsic[0, 0] = calib[\"fx\"]\n",
    "        cam_intrinsic[1, 1] = calib[\"fy\"]\n",
    "        cam_intrinsic[0, 2] = calib[\"ppx\"]\n",
    "        cam_intrinsic[1, 2] = calib[\"ppy\"]\n",
    "        return cam_intrinsic\n",
    "    \n",
    "    def get_robot_pose(self, idx, return_dict=False):\n",
    "        pose_file = os.path.join(self.base_path, \"pose_{0:04d}.json\".format(idx) )\n",
    "        with open(pose_file,\"rb\") as f_obj:\n",
    "            pose = json.load(f_obj)\n",
    "        pose_m = np.eye(4)\n",
    "        pose_m[:3, :3] = R.from_euler(\"xyz\", [pose[x] for x in ['rot_x', 'rot_y', 'rot_z']]).as_matrix()\n",
    "        pose_m[:3, 3] = [pose[x] for x in ['x', 'y', 'z']]\n",
    "        if return_dict:\n",
    "            return pose_m, pose\n",
    "        else:\n",
    "            return pose_m\n",
    "    \n",
    "    def get_rgb_file(self, idx):\n",
    "        rgb_file = os.path.join(self.base_path, \"rgb_{0:04d}.png\".format(idx) )\n",
    "        return rgb_file\n",
    "    \n",
    "    def get_depth_file(self, idx):\n",
    "        depth_file = os.path.join(self.base_path, \"depth_{0:04d}.png\".format(idx) )\n",
    "        return depth_file    \n",
    "    \n",
    "    def get_rgbdp(self, idx):\n",
    "        rgb_file = self.get_rgb_file(idx)\n",
    "        depth_file = self.get_depth_file(idx)\n",
    "        \n",
    "        pose_m, pose_d = self.get_robot_pose(idx, True)\n",
    "        # depth\n",
    "        depth_scaling = pose_d[\"depth_scaling\"]\n",
    "        rgb  = np.asarray(Image.open(rgb_file))\n",
    "        depth = np.asarray(Image.open(depth_file), dtype=np.float32) * depth_scaling\n",
    "        return rgb, depth, pose_m\n",
    "        \n",
    "    def get_cam_pose(self, idx, marker_dir=\"pose_marker_one\"):\n",
    "        marker_dir = os.path.join(self.base_path, marker_dir)\n",
    "        fn  = \"{0:08d}.json\".format(idx)\n",
    "        pose_fn = os.path.join(marker_dir, fn)\n",
    "        with open(pose_fn, \"r\") as fo:\n",
    "            T = np.array(json.load(fo))\n",
    "        return T\n",
    "    \n",
    "    def get_projection_matrix(self):\n",
    "        # returns a 4x3 projection matrix using the intrinsics\n",
    "        intr = self.get_intrinsics()\n",
    "        cam_mat = np.array([[intr['fx'], 0, intr['ppx'], 0],\n",
    "                            [0, intr['fy'], intr['ppy'], 0],\n",
    "                            [0, 0, 1, 0]])\n",
    "        assert cam_mat.shape == (3, 4)\n",
    "        return cam_mat\n",
    "    \n",
    "\n",
    "    def project(self, X):\n",
    "        \"\"\"\n",
    "        Project an (homogenous) cartesian coordinate into the camera frame.\n",
    "        \"\"\"\n",
    "        if X.shape[0] == 3:\n",
    "            if len(X.shape) == 1:\n",
    "                X = np.append(X, 1)\n",
    "            else:\n",
    "                X = np.concatenate([X, np.ones((1, X.shape[1]))], axis=0)\n",
    "\n",
    "        x = self.get_projection_matrix() @ X\n",
    "        result = np.round(x[0:2] / x[2]).astype(int)\n",
    "        width, height = self.get_intrinsics()['width'], self.get_intrinsics()['height']\n",
    "        if not (0 <= result[0] < width and 0 <= result[1] < height):\n",
    "            log.warning(\"Projected point outside of image bounds\")\n",
    "        return result[0], result[1]\n",
    "\n",
    "# vl = ViewLoader(base_path=\"TODO:insert path to wd_40 here\")\n",
    "vl = ViewLoader(base_path=\"wd_40\")\n",
    "print(\"camera calibration:\")\n",
    "camera_calibration = vl.get_K()\n",
    "K = np.array(camera_calibration)\n",
    "print(K.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout, interact\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "image, depth, pose = vl.get_rgbdp(2)\n",
    "line = ax.imshow(np.asarray(image))\n",
    "ax.set_axis_off()\n",
    "\n",
    "def update(w):\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    line.set_data(np.asarray(image))\n",
    "    fig.canvas.draw_idle()\n",
    "    # plt.imshow(np.asarray(image))\n",
    "    # plt.show()\n",
    "    \n",
    "slider_w = widgets.IntSlider(min=2, max=len(vl)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Show Marker Detection Results.\n",
    "\n",
    "To simplify things marker detection has already been run. Next we want to verify its results.\n",
    "Do this by completing the `get_projection_matrix` function in the ViewLoader.\n",
    "Then draw a coordinate frame into each image for which we have detection results.\n",
    "The coordinate frame should have axis lengths of 10cm, with x=red, y=green, and z=blue.\n",
    "This can be done using `PIL.ImageDraw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "def show_marker_pose(image, T_cam_object):\n",
    "    \"\"\"\n",
    "    draw the coordinate frame into each image for which we have detection results\n",
    "    Arguments:\n",
    "        image: image as numpy.ndarray\n",
    "        T_cam_object: transform from object into cam x_cam = T_cam_object @ x\n",
    "    Returns:\n",
    "        im: image (should be PIL.Image.Image)\n",
    "    \"\"\"\n",
    "    print(type(image))\n",
    "    print(T_cam_object.shape)\n",
    "    # TODO\n",
    "\n",
    "    #  using PIL.ImageDraw\n",
    "    \n",
    "    center = np.array([0, 0, 0, 1])\n",
    "    x = np.array([0.1, 0, 0, 1])\n",
    "    y = np.array([0, 0.1, 0, 1])\n",
    "    z = np.array([0, 0, 0.1, 1])\n",
    "\n",
    "    object_cam = T_cam_object @ center\n",
    "    x_cam = T_cam_object @ x\n",
    "    y_cam = T_cam_object @ y\n",
    "    z_cam = T_cam_object @ z\n",
    "\n",
    "    object_cam_p = vl.project(object_cam)\n",
    "    x_cam_p = vl.project(x_cam)\n",
    "    y_cam_p = vl.project(y_cam)\n",
    "    z_cam_p = vl.project(z_cam)\n",
    "\n",
    "    im = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.line(object_cam_p + x_cam_p, fill=\"red\", width=5)\n",
    "    draw.line(object_cam_p + y_cam_p, fill=\"green\", width=5)\n",
    "    draw.line(object_cam_p + z_cam_p, fill=\"blue\", width=5)\n",
    "\n",
    "    # end TODO\n",
    "    \n",
    "    # type(im) should be PIL.Image.Image\n",
    "    return im\n",
    "\n",
    "image, depth, robot_pose = vl.get_rgbdp(2)\n",
    "T_cam_object = vl.get_cam_pose(2)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "image_m = show_marker_pose(image, T_cam_object)\n",
    "line = ax.imshow(np.asarray(image_m))\n",
    "ax.set_axis_off()\n",
    "\n",
    "def update(w):\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    try:\n",
    "        T_cam_object = vl.get_cam_pose(w)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No pose estimation.\")\n",
    "        line.set_data(np.asarray(image))\n",
    "        return\n",
    "    image_m = show_marker_pose(image, T_cam_object)\n",
    "    line.set_data(np.asarray(image_m))\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "slider_w = widgets.IntSlider(min=2, max=len(vl)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tcp_marker_lists(m_dir=\"pose_marker_one\"):\n",
    "    T_robot_tcp_list = []\n",
    "    T_cam_marker_list = []\n",
    "    for i in range(len(vl)):\n",
    "        try:\n",
    "            robot_pose = vl.get_robot_pose(i)\n",
    "            cam_pose = vl.get_cam_pose(i, marker_dir=m_dir)\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            continue\n",
    "        T_robot_tcp_list.append(robot_pose)\n",
    "        T_cam_marker_list.append(cam_pose)\n",
    "\n",
    "    return np.array(T_robot_tcp_list), np.array(T_cam_marker_list)\n",
    "\n",
    "plot_o3d = True\n",
    "# plot_o3d = False\n",
    "if plot_o3d:\n",
    "    T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "    mesh_frames = []\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n",
    "        mesh_frame.transform(np.linalg.inv(T_robot_tcp))\n",
    "        mesh_frames.append(mesh_frame)\n",
    "        \n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.20)\n",
    "        mesh_frame.transform(T_cam_marker)\n",
    "        mesh_frames.append(mesh_frame)\n",
    "\n",
    "    o3d.visualization.draw_geometries(mesh_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Merged Pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "first_rgb = Image.open(vl.get_rgb_file(2))\n",
    "K_o3d = o3d.camera.PinholeCameraIntrinsic()\n",
    "K_o3d.set_intrinsics(first_rgb.size[1], first_rgb.size[0],\n",
    "                     K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "\n",
    "pcd_list = []\n",
    "for i in range(2,len(vl)):\n",
    "    try:\n",
    "        rgb_file = Image.open(vl.get_rgb_file(i))\n",
    "        depth_file = Image.open(vl.get_depth_file(i))\n",
    "        T_c = vl.get_cam_pose(i)\n",
    "        T_r = vl.get_robot_pose(i)\n",
    "        depth_scaling = vl.get_robot_pose(i, return_dict=True)[1][\"depth_scaling\"]\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        continue\n",
    "    \n",
    "    rgb = o3d.geometry.Image(np.array(rgb_file))\n",
    "    depth = o3d.geometry.Image(np.array(depth_file).astype(np.uint16))\n",
    "    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(rgb, depth,\n",
    "                                          depth_scale=1.0/depth_scaling, depth_trunc=1.0,\n",
    "                                          convert_rgb_to_intensity=False)\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, K_o3d)\n",
    "    \n",
    "    #T_est = T_r @ T_calib\n",
    "    T_est = np.linalg.inv(T_c)\n",
    "    pcd.transform(T_est)\n",
    "    pcd_list.append(pcd)\n",
    "\n",
    "# sum pointclouds for easier visualization\n",
    "pcd_all = pcd_list[0]\n",
    "for pcd_cur in pcd_list[1:]:\n",
    "    pcd_all += pcd_cur\n",
    "o3d.visualization.draw_geometries([pcd_all])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: improve the pointcloud\n",
    "- remove the gripper\n",
    "- unify the points on the surface\n",
    "- extract a mesh\n",
    "\n",
    "Just try out many things. You can find some inspiration in [tutorials](http://www.open3d.org/docs/0.8.0/index.html#tutorial-index). Make sure the docs version and your installed version match, since some functionality has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40de0b164a2ead70a1213ee87ce739cfc5594d2111c42683eb8f5e0739ba5537"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
